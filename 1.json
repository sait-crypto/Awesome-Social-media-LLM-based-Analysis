{
  "papers": [
    {
      "doi": "10.1609/aaai.v38i8.28787",
      "title": "CAMEL: Capturing metaphorical alignment with context disentangling for multimodal emotion recognition",
      "authors": "Linhao Zhang, Li Jin, Guangluan Xu, Xiaoyu Li, Cai Xu, Kaiwen Wei, Nayu Liu, Haonan Liu",
      "date": "2024-03-24",
      "category": "Other;Generation;Social Media Security",
      "summary_motivation": "",
      "summary_innovation": "",
      "summary_method": "",
      "summary_conclusion": "",
      "summary_limitation": "",
      "paper_url": "https://ojs.aaai.org/index.php/AAAI/article/view/28787",
      "project_url": "",
      "conference": "AAAI",
      "title_translation": "",
      "analogy_summary": "TLDR: This work proposes to leverage a conditional generative approach for capturing metaphorical analogies in MER, and incorporates a disentangled contrastive matching mechanism, which undergoes curricular adjustment to regulate its intensity during the learning process.",
      "pipeline_image": "",
      "abstract": "Understanding the emotional polarity of multimodal content with metaphorical characteristics, such as memes, poses a significant challenge in Multimodal Emotion Recognition (MER). Previous MER researches have overlooked the phenomenon of metaphorical alignment in multimedia content, which involves non-literal associations between concepts to convey implicit emotional tones.  Metaphor-agnostic MER methods may be misinformed by the isolated unimodal emotions, which are distinct from the real emotions blended in multimodal metaphors. Moreover, contextual semantics can further affect the emotions associated with similar metaphors, leading to the challenge of maintaining contextual compatibility. To address the issue of metaphorical alignment in MER, we propose to leverage a conditional generative approach for capturing metaphorical analogies. Our approach formulates schematic prompts and corresponding references based on theoretical foundations, which allows the model to better grasp metaphorical nuances. In order to maintain contextual sensitivity, we incorporate a disentangled contrastive matching mechanism, which undergoes curricular adjustment to regulate its intensity during the learning process. The automatic and human evaluation experiments on two benchmarks prove that, our model provides considerable and stable improvements in recognizing multimodal emotion with metaphor attributes.",
      "contributor": "",
      "notes": "",
      "show_in_readme": true,
      "status": "unread",
      "submission_time": "",
      "conflict_marker": false,
      "invalid_fields": "",
      "is_placeholder": false
    },
    {
      "doi": "10.1609/icwsm.v19i1.35801",
      "title": "Extracting affect aggregates from longitudinal social media data with temporal adapters for large language models",
      "authors": "Georg Ahnert, Max Pellert, David Garcia, Markus Strohmaier",
      "date": "2025-06-07",
      "category": "Generation",
      "summary_motivation": "",
      "summary_innovation": "",
      "summary_method": "",
      "summary_conclusion": "",
      "summary_limitation": "",
      "paper_url": "https://ojs.aaai.org/index.php/ICWSM/article/view/35801",
      "project_url": "",
      "conference": "Proceedings of the International AAAI Conference on Web and Social Media",
      "title_translation": "",
      "analogy_summary": "TLDR: This work fine-tune Temporal Adapters for Llama 3 8B on full timelines from a panel of British Twitter users and extract longitudinal aggregates of emotions and attitudes with established questionnaires, and extends the analysis of affect in LLMs to a longitudinal setting through Temporal Adapters.",
      "pipeline_image": "",
      "abstract": "This paper proposes temporally aligned Large Language Models (LLMs) as a tool for longitudinal analysis of social media data. We fine-tune Temporal Adapters for Llama 3 8B on full timelines from a panel of British Twitter users and extract longitudinal aggregates of emotions and attitudes with established questionnaires. We focus our analysis on the beginning of the COVID-19 pandemic that had a strong impact on public opinion and collective emotions. We validate our estimates against representative British survey data and find strong positive and significant correlations for several collective emotions. The estimates obtained are robust across multiple training seeds and prompt formulations, and in line with collective emotions extracted using a traditional classification model trained on labeled data. We demonstrate the flexibility of our method on questions of public opinion for which no pre-trained classifier is available. Our work extends the analysis of affect in LLMs to a longitudinal setting through Temporal Adapters. It enables flexible and new approaches to the longitudinal analysis of social media data.",
      "contributor": "",
      "notes": "",
      "show_in_readme": true,
      "status": "unread",
      "submission_time": "",
      "conflict_marker": false,
      "invalid_fields": "",
      "is_placeholder": false
    },
    {
      "doi": "10.1145/3746027.3754828",
      "title": "From individuals to crowds: Dual-level public response prediction in social media",
      "authors": "Jinghui Zhang, Kaiyang Wan, Longwei Xu, Ao Li, Zongfang Liu, Xiuying Chen",
      "date": "2025-10-27",
      "category": "Understanding",
      "summary_motivation": "",
      "summary_innovation": "",
      "summary_method": "",
      "summary_conclusion": "",
      "summary_limitation": "",
      "paper_url": "https://dl.acm.org/doi/10.1145/3746027.3754828",
      "project_url": "",
      "conference": "MM '25",
      "title_translation": "",
      "analogy_summary": "TLDR: Experimental results on SentiWeibo and related LaMP benchmark demonstrate that SocialAlign surpasses strong baselines, showing improved accuracy, interpretability, and generalization in public response prediction.",
      "pipeline_image": "",
      "abstract": "Public response prediction is critical for understanding how individuals or groups might react to specific events, policies, or social phenomena, making it highly valuable for crisis management, policy-making, and social media analysis. However, existing works face notable limitations. First, they lack micro-level personalization, producing generic responses that ignore individual user preferences. Moreover, they overlook macro-level sentiment distribution and only deal with individual-level sentiment, constraining them from analyzing broader societal trends and group sentiment dynamics. To address these challenges, we propose SocialAlign, a unified framework that predicts real-world responses at both micro and macro levels in social contexts. At the micro level, SocialAlign employs SocialLLM with an articulate Personalized Analyze-Compose LoRA (PAC-LoRA) structure, which deploys specialized expert modules for content analysis and response generation across diverse topics and user profiles, enabling the generation of personalized comments with corresponding sentiments. At the macro level, it models group sentiment distributions and aligns predictions with real-world sentiment trends derived from social media data. To evaluate SocialAlign in real-world scenarios, we introduce SentiWeibo, a large-scale dataset curated from authentic social interactions on the Weibo platform. Experimental results on our SentiWeibo and related LaMP benchmark demonstrate that SocialAlign surpasses strong baselines, showing improved accuracy, interpretability, and generalization in public response prediction. We hope our work inspires further research in public response prediction and computational social science: https://github.com/Znull-1220/SocialAlign.",
      "contributor": "",
      "notes": "",
      "show_in_readme": true,
      "status": "unread",
      "submission_time": "",
      "conflict_marker": false,
      "invalid_fields": "",
      "is_placeholder": false
    },
    {
      "doi": "10.1145/3543873.3587605",
      "title": "LLMs to the Moon? Reddit Market Sentiment Analysis with Large Language Models",
      "authors": "Xiang Deng, Vasilisa Bashlovkina, Feng Han, Simon Baumgartner, Michael Bendersky",
      "date": "2023-04-30",
      "category": "Social Media Security",
      "summary_motivation": "阿斯蒂芬的地方",
      "summary_innovation": "",
      "summary_method": "",
      "summary_conclusion": "",
      "summary_limitation": "",
      "paper_url": "https://dl.acm.org/doi/10.1145/3543873.3587605",
      "project_url": "",
      "conference": "WWW '23 Companion",
      "title_translation": "",
      "analogy_summary": "TLDR: This case study conducts a case study approaching market sentiment analysis on social media content with semi-supervised learning using a large language model (LLM), finding that prompting the LLM to produce Chain-of-Thought summaries and forcing it through several reasoning paths helps generate more stable and accurate labels.",
      "pipeline_image": "",
      "abstract": "Market sentiment analysis on social media content requires knowledge of both financial markets and social media jargon, which makes it a challenging task for human raters. The resulting lack of high-quality labeled data stands in the way of conventional supervised learning methods. In this work, we conduct a case study approaching this problem with semi-supervised learning using a large language model (LLM). We select Reddit as the target social media platform due to its broad coverage of topics and content types. Our pipeline first generates weak financial sentiment labels for Reddit posts with an LLM and then uses that data to train a small model that can be served in production. We find that prompting the LLM to produce Chain-of-Thought summaries and forcing it through several reasoning paths helps generate more stable and accurate labels, while training the student model using a regression loss further improves distillation quality. With only a handful of prompts, the final model performs on par with existing supervised models. Though production applications of our model are limited by ethical considerations, the model’s competitive performance points to the great potential of using LLMs for tasks that otherwise require skill-intensive annotation.",
      "contributor": "",
      "notes": "",
      "show_in_readme": true,
      "status": "unread",
      "submission_time": "",
      "conflict_marker": false,
      "invalid_fields": "",
      "is_placeholder": false
    }
  ],
  "meta": {
    "generated_at": "2026-01-26 18:40:18"
  }
}