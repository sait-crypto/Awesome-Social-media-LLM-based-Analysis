"""
é€šç”¨å·¥å…·å‡½æ•°
"""
import urllib.parse
import shutil
import os
import re
import json
from typing import List, Dict, Any, Optional,Tuple
from datetime import datetime
from pathlib import Path



def validate_url(url: str) -> bool:
    """éªŒè¯URLæ ¼å¼"""
    if not url:
        return False
    
    url_pattern = re.compile(
        r'^https?://'  # http:// or https://
        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z0-9-]{2,}\.?|'  # domain...
        r'localhost|'  # localhost...
        r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
        r'(?::\d+)?'  # optional port
        r'(?:/?|[/?]\S+)$', re.IGNORECASE)
    
    return bool(re.match(url_pattern, url))


def clean_doi(doi: str, conflict_marker: str = None) -> str:
    """æ¸…ç†DOIï¼Œç§»é™¤URLéƒ¨åˆ†å’Œå†²çªæ ‡è®°"""
    if not doi:
        return ""
    
    # å¦‚æœæä¾›äº†å†²çªæ ‡è®°ï¼Œå…ˆç§»é™¤
    if conflict_marker:
        doi = doi.replace(conflict_marker, "").strip()
    
    doi = doi.strip()
    
    # ç§»é™¤å¸¸è§çš„URLå‰ç¼€
    prefixes = ['https://doi.org/', 'http://doi.org/', 
                'https://dx.doi.org/', 'http://dx.doi.org/',
                'doi:', 'DOI:']
    
    for prefix in prefixes:
        if doi.lower().startswith(prefix.lower()):
            doi = doi[len(prefix):]
            break
    #è¿›ä¸€æ­¥ï¼Œæ¸…é™¤doi/å­—ä¸²åŠå‰é¢çš„å†…å®¹
    match = re.search(r"doi/(.*)", doi, flags=re.IGNORECASE)
    doi = match.group(1) if match else doi
    return doi


def validate_doi(doi: str, check_format: bool = True, conflict_marker: str = None) -> Tuple[bool, str]:
    """éªŒè¯DOIæ ¼å¼ï¼Œè¿”å›(æ˜¯å¦æœ‰æ•ˆ, æ¸…ç†åçš„DOI)"""
    if not doi:
        return (False, "")
    
    # æ¸…ç†DOI
    cleaned_doi = clean_doi(doi, conflict_marker)
    
    if not cleaned_doi:
        return (False, "")
    
    # å¦‚æœä¸éœ€è¦æ ¼å¼éªŒè¯ï¼Œç›´æ¥è¿”å›
    if not check_format:
        return (True, cleaned_doi)
    
    # DOIæ­£åˆ™è¡¨è¾¾å¼
    doi_pattern = re.compile(r'^10\.\d{4,9}/[-._;()/:A-Z0-9]+$', re.IGNORECASE)
    
    if bool(re.match(doi_pattern, cleaned_doi)):
        return (True, cleaned_doi)
    else:
        return (False, cleaned_doi)


def format_authors(authors: str, max_length: int = 150) -> str:
    """æ ¼å¼åŒ–ä½œè€…åˆ—è¡¨"""
    if not authors:
        return ""
    
    authors = str(authors)
    # æ¸…ç†å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
    authors = ' '.join(authors.split())
    
    # å¦‚æœä½œè€…åˆ—è¡¨å¤ªé•¿ï¼Œæˆªæ–­
    if len(authors) > max_length:
        # å°è¯•åœ¨é€—å·å¤„æˆªæ–­
        parts = authors.split(',')
        truncated = parts[0]
        
        for i in range(1, len(parts)):
            if len(truncated + ', ' + parts[i]) <= max_length - 3:  # ä¸º"..."ç•™ç©ºé—´
                truncated += ', ' + parts[i]
            else:
                truncated += ', ...'
                break
        
        return truncated
    
    return authors


def validate_authors(authors: str, max_length: int = 150) -> Tuple[bool, str]:
    """éªŒè¯ä½œè€…æ ¼å¼ï¼Œè¿”å›(æ˜¯å¦æœ‰æ•ˆ, æ ¼å¼åŒ–åçš„ä½œè€…)"""
    if not authors:
        return (False, "")
    
    formatted = format_authors(authors, max_length)
    return (len(formatted) > 0, formatted)


def normalize_pipeline_image(path: str, figure_dir: str = "figures") -> str:
    """
    è§„èŒƒåŒ–pipelineå›¾ç‰‡è·¯å¾„
    è¾“å…¥å¯ä»¥æ˜¯ï¼š1) æ–‡ä»¶å 2) ç›¸å¯¹è·¯å¾„ 3) ç»å¯¹è·¯å¾„
    è¾“å‡ºï¼šç›¸å¯¹äºé¡¹ç›®æ ¹çš„è·¯å¾„ï¼Œå¦‚ "figures/image.png"
    """
    if not path or not str(path).strip():
        return ""
    
    path_s = str(path).strip()
    
    # ç»Ÿä¸€ä½¿ç”¨æ­£æ–œæ 
    path_s = path_s.replace('\\', '/')
    figure_dir = figure_dir.replace('\\', '/').rstrip('/')
    # å¦‚æœ figure_dir æ˜¯ç»å¯¹è·¯å¾„ï¼ˆæ¥è‡ª setting.configï¼‰ï¼Œå–å…¶ basename ä»¥ä¿è¯è¿”å›å€¼ä¸ºç›¸å¯¹è·¯å¾„ï¼ˆä¾‹å¦‚ "figures"ï¼‰
    if os.path.isabs(figure_dir):
        figure_dir = os.path.basename(figure_dir)
    
    # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œæå–æ–‡ä»¶åå¹¶æ”¾åˆ°ç›¸å¯¹çš„ figure_dir ä¸‹
    if os.path.isabs(path_s):
        filename = os.path.basename(path_s)
        return f"{figure_dir}/{filename}"
    
    # å¦‚æœå·²ç»æ˜¯ç›¸å¯¹è·¯å¾„ä¸”ä»¥figure_dirå¼€å¤´ï¼Œç›´æ¥è¿”å›
    if path_s.startswith(figure_dir + '/'):
        return path_s
    
    # å¦‚æœåªæ˜¯æ–‡ä»¶åï¼ˆä¸åŒ…å«è·¯å¾„åˆ†éš”ç¬¦ï¼‰ï¼Œæ·»åŠ figure_dirå‰ç¼€
    if '/' not in path_s and '\\' not in path_s:
        return f"{figure_dir}/{path_s}"
    
    # å…¶ä»–æƒ…å†µï¼šæå–æ–‡ä»¶åå¹¶æ”¾åˆ°figure_dirä¸‹
    filename = os.path.basename(path_s)
    return f"{figure_dir}/{filename}"


def validate_pipeline_image(path: str, figure_dir: str = "figures") -> Tuple[bool, str]:
    """
    éªŒè¯ pipeline å›¾ç‰‡ï¼ˆæ”¯æŒå¤šå›¾ï¼‰ï¼Œè¿”å› (æ˜¯å¦æœ‰æ•ˆ, è§„èŒƒåŒ–åçš„è·¯å¾„æˆ–å¤šå›¾ä»¥";"è¿æ¥çš„å­—ç¬¦ä¸²)
    - å…è®¸ä½¿ç”¨åˆ†éš”ç¬¦ `;` æˆ–ä¸­æ–‡ `ï¼›` è¾“å…¥å¤šå¼ å›¾ç‰‡
    - æœ€å¤šå…è®¸ 3 å¼ å›¾ç‰‡ï¼›è¶…è¿‡åˆ™è¿”å› False
    - æ¯å¼ å›¾ç‰‡ä½¿ç”¨ `normalize_pipeline_image` è§„èŒƒåŒ–å¹¶éªŒè¯æ‰©å±•åä¸ä½äº figure_dir ä¸‹
    """
    if not path or not str(path).strip():
        return (True, "")  # å…è®¸ä¸ºç©º

    path_s = str(path).strip()

    # æ”¯æŒå¤šåˆ†éš”ç¬¦ï¼š; æˆ– ä¸­æ–‡ ï¼›
    parts = [p.strip() for p in re.split(r'[;ï¼›]', path_s) if p.strip()]

    if not parts:
        return (True, "")

    # é™åˆ¶æœ€å¤š 3 å¼ å›¾ç‰‡
    if len(parts) > 3:
        # ä»è¿”å›è§„èŒƒåŒ–çš„å‰3é¡¹ä»¥ä¾¿æç¤ºï¼Œä½†åˆ¤å®šä¸ºæ— æ•ˆ
        normalized_parts = [normalize_pipeline_image(p, figure_dir) for p in parts[:3]]
        return (False, ";".join(normalized_parts))

    normalized_parts = []
    valid_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.svg'}

    # è§„èŒƒåŒ– figure_dir ç”¨äºæ¯”è¾ƒ
    fig_dir_norm = figure_dir.replace('\\', '/').rstrip('/')
    if os.path.isabs(fig_dir_norm):
        fig_dir_norm = os.path.basename(fig_dir_norm)

    for p in parts:
        normalized = normalize_pipeline_image(p, figure_dir)
        ext = os.path.splitext(normalized)[1].lower()
        if ext not in valid_extensions:
            return (False, normalized)
        if not normalized.startswith(fig_dir_norm + '/'):
            return (False, normalized)
        normalized_parts.append(normalized)

    # ä»¥åˆ†å·åˆ†éš”è¿”å›è§„èŒƒåŒ–åçš„è·¯å¾„
    return (True, ";".join(normalized_parts))

def validate_date(date_str: Any) -> Tuple[bool, str]:
    """
    éªŒè¯å¹¶è§„èŒƒåŒ–æ—¥æœŸæ ¼å¼
    æµç¨‹ï¼š
    1. è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶å»é™¤ç©ºç™½
    2. å°è¯•æ¸…æ´—ï¼ˆå»é™¤æ—¶åˆ†ç§’ï¼Œç»Ÿä¸€åˆ†éš”ç¬¦ï¼‰
    3. éªŒè¯æ˜¯å¦ç¬¦åˆæ ¼å¼ï¼Œæ”¯æŒæ—¥ç¼ºçœå’Œæ—¥æœˆä»½ç¼ºçœ
    
    è¿”å›: (æ˜¯å¦æœ‰æ•ˆ, è§„èŒƒåŒ–åçš„æ—¥æœŸå­—ç¬¦ä¸²)
    """
    if date_str is None:
        return (True, "")
        
    # 1. è½¬å­—ç¬¦ä¸²å¹¶å»é™¤é¦–å°¾ç©ºæ ¼
    s_val = str(date_str).strip()
    if not s_val:
        return (True, "")

    # æ•è·åŸå§‹å€¼ï¼Œé˜²æ­¢åç»­splitç©ºæ ¼æ—¶æˆªæ–­éæ ‡å‡†æ—¥æœŸï¼ˆå¦‚ "Oct 27, 2025"ï¼‰
    original_val = s_val

    # 2. å»é™¤æ—¶é—´éƒ¨åˆ†
    if ' ' in s_val:
        s_val = s_val.split(' ')[0]
    
    try:
        final_str = ""
        
        # 3. ç»Ÿä¸€åˆ†éš”ç¬¦ï¼šå°† / å’Œ . æ›¿æ¢ä¸º -
        s_val = re.sub(r'[/\.]', '-', s_val)
        
        # 4. å¤„ç†çº¯æ•°å­—æ ¼å¼
        if s_val.isdigit():
            if len(s_val) == 4:   # YYYY -> YYYY
                final_str = s_val
            elif len(s_val) == 6: # YYYYMM -> YYYY-MM
                final_str = f"{s_val[:4]}-{s_val[4:]}"
            elif len(s_val) == 8: # YYYYMMDD -> YYYY-MM-DD
                final_str = f"{s_val[:4]}-{s_val[4:6]}-{s_val[6:]}"
            else:
                return (False, original_val)
        
        # 5. å¤„ç†å¸¦åˆ†éš”ç¬¦çš„æ ¼å¼
        else:
            parts = s_val.split('-')
            # è¿‡æ»¤ç©ºä¸²
            parts = [p for p in parts if p]
            
            if not parts:
                return (False, original_val)
            
            year = parts[0]
            if len(year) != 4 or not year.isdigit():
                return (False, original_val)

            if len(parts) == 1:   # YYYY
                final_str = year
            elif len(parts) == 2: # YYYY-MM
                month = int(parts[1])
                if not (1 <= month <= 12): return (False, original_val)
                final_str = f"{year}-{month:02d}"
            elif len(parts) >= 3: # YYYY-MM-DD
                month = int(parts[1])
                day = int(parts[2])
                # åˆ©ç”¨ datetime éªŒè¯æ—¥æœŸçš„åˆæ³•æ€§
                try:
                    datetime(int(year), month, day)
                except ValueError:
                    return (False, original_val)
                final_str = f"{year}-{month:02d}-{day:02d}"
            else:
                return (False, original_val)

        return (True, final_str)

    except (ValueError, IndexError):
        return (False, original_val)


def validate_invalid_fields(invalid_fields: str) -> Tuple[bool, str]:
    """
    éªŒè¯ invalid_fields å­—æ®µ
    invalid_fields æ˜¯é€—å·æˆ–ä¸­æ–‡é€—å·åˆ†éš”çš„å­—æ®µ order åˆ—è¡¨ï¼Œæ¯ä¸ªéƒ½åº”è¯¥æ˜¯éè´Ÿæ•´æ•°ï¼ˆ>= 0ï¼‰
    
    æµç¨‹ï¼š
    1. å¦‚æœä¸ºç©ºï¼Œè¿”å›æœ‰æ•ˆ
    2. æŒ‰ , æˆ– ï¼Œ åˆ†å‰²
    3. éªŒè¯æ¯ä¸ªéƒ¨åˆ†æ˜¯å¦éƒ½æ˜¯éè´Ÿæ•´æ•°
    4. è¿”å›éªŒè¯ç»“æœå’Œé”™è¯¯ä¿¡æ¯
    
    å‚æ•°:
        invalid_fields: é€—å·åˆ†éš”çš„å­—æ®µorderåˆ—è¡¨
    
    è¿”å›: (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
    """
    if not invalid_fields or str(invalid_fields).strip() == "":
        return (True, "")
    
    invalid_fields_str = str(invalid_fields).strip()
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŒ‰ , æˆ– ï¼Œ åˆ†å‰²
    parts = re.split(r'[,ï¼Œ]', invalid_fields_str)
    
    # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
    parts = [p.strip() for p in parts if p.strip()]
    
    if not parts:
        return (True, "")
    
    # éªŒè¯æ¯ä¸ªéƒ¨åˆ†æ˜¯å¦éƒ½æ˜¯éè´Ÿæ•´æ•°
    for part in parts:
        # æ£€æŸ¥æ˜¯å¦å…¨æ˜¯æ•°å­—
        if not part.isdigit():
            return (False, f"invalid_fields ä¸­å«æœ‰éæ•´æ•°å€¼: '{part}'ï¼ˆåº”è¯¥æ˜¯éè´Ÿæ•´æ•°ï¼‰")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯éè´Ÿæ•´æ•°ï¼ˆå³ >= 0ï¼‰
        try:
            value = int(part)
            if value < 0:
                return (False, f"invalid_fields ä¸­å«æœ‰è´Ÿæ•°: {value}ï¼ˆåº”è¯¥æ˜¯éè´Ÿæ•´æ•°ï¼‰")
        except ValueError:
            return (False, f"invalid_fields ä¸­å«æœ‰æ— æ³•è½¬æ¢ä¸ºæ•´æ•°çš„å€¼: '{part}'")
    
    return (True, "")

    
def extract_doi_from_url(url: str) -> Optional[str]:
    """ä»URLä¸­æå–DOI"""
    if not url:
        return None
    
    # åŒ¹é…DOI URL
    doi_patterns = [
        r'doi\.org/(10\.\d{4,9}/[-._;()/:A-Z0-9]+)',
        r'dx\.doi\.org/(10\.\d{4,9}/[-._;()/:A-Z0-9]+)',
        r'doi:(10\.\d{4,9}/[-._;()/:A-Z0-9]+)'
    ]
    
    for pattern in doi_patterns:
        match = re.search(pattern, url, re.IGNORECASE)
        if match:
            return match.group(1)
    
    return None


def ensure_directory(path: str) -> bool:
    """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
    try:
        Path(path).mkdir(parents=True, exist_ok=True)
        return True
    except Exception as e:
        print(f"åˆ›å»ºç›®å½•å¤±è´¥ {path}: {e}")
        return False




def truncate_text(text: str, max_length: int, ellipsis: str = "...") -> str:
    """æˆªæ–­æ–‡æœ¬ï¼Œä¿ç•™æœ€å¤§é•¿åº¦"""
    if not text:
        return ""
    
    if len(str(text)) <= max_length:
        return text
    
    return text[:max_length - len(ellipsis)] + ellipsis



def get_current_timestamp() -> str:
    """è·å–å½“å‰æ—¶é—´æˆ³"""
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def compare_papers(paper1: Dict, paper2: Dict, key_fields: List[str] = None) -> List[str]:
    """æ¯”è¾ƒä¸¤ç¯‡è®ºæ–‡çš„å·®å¼‚ï¼Œè¿”å›å·®å¼‚å­—æ®µåˆ—è¡¨"""
    if key_fields is None:
        key_fields = ['doi', 'title', 'authors']
    
    differences = []
    
    for field in key_fields:
        val1 = paper1.get(field, "")
        val2 = paper2.get(field, "")
        
        if str(val1).strip().lower() != str(val2).strip().lower():
            differences.append(field)
    
    return differences


def merge_paper_data(existing: Dict, new: Dict, prefer_new: bool = True) -> Dict:
    """åˆå¹¶ä¸¤ç¯‡è®ºæ–‡çš„æ•°æ®"""
    result = existing.copy()
    
    for key, new_value in new.items():
        existing_value = existing.get(key, "")
        
        if not existing_value or (prefer_new and new_value):
            result[key] = new_value
    
    return result


def _escape_md_text(text: str) -> str:
    """å¯¹ Markdown é“¾æ¥æ–‡æœ¬åšåŸºç¡€è½¬ä¹‰ï¼ˆä¸­æ‹¬å·/åæ–œæ /æ¢è¡Œï¼‰"""
    if text is None:
        return ""
    s = str(text)
    s = s.replace("\\", "\\\\")
    s = s.replace("\n", " ")
    s = s.replace("]", "\\]")
    s = s.replace("[", "\\[")
    return s

def create_hyperlink(text: str, url: str) -> str:
    """
    ç”Ÿæˆ Markdown é£æ ¼è¶…é“¾æ¥ï¼š [text](url)
    - å½“ url ä¸ºç©ºæˆ– None æ—¶ä»…è¿”å›æ–‡æœ¬
    - å¯¹æ˜¾ç¤ºæ–‡æœ¬åšåŸºç¡€è½¬ä¹‰ä»¥é¿å…ç ´å Markdown
    """
    if not url:
        return _escape_md_text(text)
    # ä¿è¯ URL ä¸­çš„ç©ºæ ¼ç­‰è¢«å®‰å…¨ç¼–ç ï¼Œä½†ä¿ç•™å¸¸è§ URL å­—ç¬¦
    try:
        parsed = urllib.parse.urlsplit(url)
        if not parsed.scheme:
            # è‹¥ç¼ºå°‘ schemeï¼Œå‡å®šä¸º https
            url = "https://" + url
        # åªå¯¹ URL çš„è·¯å¾„/æŸ¥è¯¢éƒ¨åˆ†åš quoteï¼Œä¿ç•™ :// å’Œä¸»æœº
        url = urllib.parse.urlunsplit(parsed._replace(path=urllib.parse.quote(parsed.path, safe="/"), query=urllib.parse.quote_plus(parsed.query, safe="=&")) )
    except Exception:
        url = url.replace(" ", "%20")
    return f"[{_escape_md_text(text)}]({url})"


def escape_markdown(text: str) -> str:
    """è½¬ä¹‰Markdownç‰¹æ®Šå­—ç¬¦"""

    if not text:
        return ""
    text = str(text)
    # éœ€è¦è½¬ä¹‰çš„Markdownå­—ç¬¦
    special_chars = ['\\', '`', '*', '_', '{', '}', '[', ']', '(', ')', '#', '+', '-', '.', '!']
    
    for char in special_chars:
        text = text.replace(char, '\\' + char)
    
    return text
def escape_markdown_base(text: str) -> str:
    """åªè½¬ä¹‰å®¹æ˜“å¼•èµ·é—®é¢˜çš„Markdownå­—ç¬¦ï¼šåæ–œæ ã€æ–¹æ‹¬å·ã€åœ†æ‹¬å·"""
    if not text:
        return ""
    text = str(text)
    special_chars = ['\\', '[', ']', '(', ')']
    
    for char in special_chars:
        text = text.replace(char, '\\' + char)
    
    return text

def sanitize_filename(filename: str) -> str:
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
    # ç§»é™¤éæ³•å­—ç¬¦
    illegal_chars = ['<', '>', ':', '"', '/', '\\', '|', '?', '*']
    for char in illegal_chars:
        filename = filename.replace(char, '_')
    
    # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºæ ¼å’Œç‚¹
    filename = filename.strip('. ')
    
    # é™åˆ¶é•¿åº¦
    if len(filename) > 255:
        name, ext = os.path.splitext(filename)
        filename = name[:255 - len(ext)] + ext
    
    return filename


def validate_figure(path: str, figure_dir: str) -> bool:
    """
    éªŒè¯ pipeline å›¾åƒè·¯å¾„/æ–‡ä»¶åæ˜¯å¦åˆæ³•ï¼š
    - æ‰©å±•åå¿…é¡»æ˜¯ .jpg/.jpeg/.png
    - å¿…é¡»ä¸ºç›¸å¯¹è·¯å¾„ï¼Œä¸”ä½äº figure_dir ä¸‹ï¼ˆå¦‚æœç»™çš„æ˜¯å¸¦ç›®å½•çš„è·¯å¾„ï¼‰
    - å½“åªç»™æ–‡ä»¶åæ—¶è§†ä¸ºæœ‰æ•ˆï¼ˆåç»­ä¼šè¡¥å…¨ä¸º figure_dir/filenameï¼‰
    
    æ³¨æ„ï¼šè¿™é‡ŒåªéªŒè¯æ ¼å¼å’Œä½ç½®ï¼Œä¸éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    """
    if not path or not str(path).strip():
        return False
    
    path_s = str(path).strip()
    
    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    valid_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.svg'}
    ext = os.path.splitext(path_s)[1].lower()
    if ext not in valid_extensions:
        return False
    
    # è§„èŒƒåŒ–è·¯å¾„ï¼Œç¡®ä¿ä½¿ç”¨æ­£æ–œæ 
    path_s = path_s.replace('\\', '/')
    
    # å¦‚æœåªæ˜¯æ–‡ä»¶åï¼ˆä¸åŒ…å«è·¯å¾„åˆ†éš”ç¬¦ï¼‰ï¼Œç›´æ¥è¿”å›True
    if '/' not in path_s:
        return True
    
    # å¦‚æœåŒ…å«è·¯å¾„ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨figure_dirä¸‹
    # è§„èŒƒåŒ–figure_dir
    fig_dir_norm = figure_dir.replace('\\', '/').rstrip('/')
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦ä»¥figure_dirå¼€å¤´
    if not path_s.startswith(fig_dir_norm + '/'):
        return False
    
    return True


def normalize_figure_path(path: str, figure_dir: str) -> str:
    """
    æŠŠç”¨æˆ·è¾“å…¥çš„å›¾åƒå/è·¯å¾„è§„èŒƒä¸ºç›¸å¯¹äºä»“åº“æ ¹çš„è·¯å¾„ï¼ˆä½¿ç”¨æ­£æ–œæ ï¼‰ã€‚
    - è‹¥ä»…æ˜¯æ–‡ä»¶åï¼šè¿”å› figure_dir/filename
    - è‹¥å·²æœ‰è·¯å¾„ä¸”ä»¥ figure_dir å¼€å¤´ï¼šè¿”å›è§„èŒƒåŒ–çš„è·¯å¾„
    - è‹¥å·²æœ‰è·¯å¾„ä½†ä¸ä»¥ figure_dir å¼€å¤´ï¼šæå–æ–‡ä»¶åå¹¶æ”¾åˆ° figure_dir ä¸‹
    
    æ³¨æ„ï¼šè¿™é‡Œä¸éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œåªåšè·¯å¾„è§„èŒƒåŒ–
    """
    if not path or not str(path).strip():
        return ""
    
    path_s = str(path).strip()
    
    # ç»Ÿä¸€ä½¿ç”¨æ­£æ–œæ 
    path_s = path_s.replace('\\', '/')
    fig_dir_norm = figure_dir.replace('\\', '/').rstrip('/')
    
    # å¦‚æœåªæ˜¯æ–‡ä»¶åï¼ˆä¸åŒ…å«è·¯å¾„åˆ†éš”ç¬¦ï¼‰ï¼Œæ·»åŠ figure_dirå‰ç¼€
    if '/' not in path_s and '\\' not in path_s:
        return f"{fig_dir_norm}/{path_s}"
    
    # å¦‚æœå·²ç»ä»¥figure_dirå¼€å¤´ï¼Œç›´æ¥è¿”å›è§„èŒƒåŒ–çš„è·¯å¾„
    if path_s.startswith(fig_dir_norm + '/'):
        # ç¡®ä¿åªæœ‰ä¸€ä¸ªfigure_dirå‰ç¼€
        parts = path_s.split('/')
        if parts[0] == fig_dir_norm:
            return path_s
        else:
            # å¦‚æœfigure_diråŒ…å«å¤šçº§ç›®å½•ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            return f"{fig_dir_norm}/{os.path.basename(path_s)}"
    
    # å…¶ä»–æƒ…å†µï¼šæå–æ–‡ä»¶åå¹¶æ”¾åˆ°figure_dirä¸‹
    return f"{fig_dir_norm}/{os.path.basename(path_s)}"



def figure_exists_in_repo(figure_path: str, project_root: str = None) -> bool:
    """
    æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨äºä»“åº“ä¸­
    - figure_path: è§„èŒƒåŒ–åçš„å›¾ç‰‡è·¯å¾„
    - project_root: é¡¹ç›®æ ¹ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•
    """
    if not figure_path:
        return False
    
    if project_root is None:
        from src.core.config_loader import get_config_instance
        project_root = str(get_config_instance().project_root)
    
    # æ„å»ºå®Œæ•´è·¯å¾„
    full_path = os.path.join(project_root, figure_path)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    return os.path.isfile(full_path)


def backup_file(filepath: str, backup_dir: str) -> Optional[str]:
    """
    ç»Ÿä¸€å¤‡ä»½æ–‡ä»¶/æ–‡ä»¶å¤¹å‡½æ•°ï¼ˆå…¼å®¹æ–‡ä»¶å’Œæ–‡ä»¶å¤¹ï¼‰
    é€»è¾‘ï¼š
    - æ–‡ä»¶ï¼šåŸæ–‡ä»¶å(æ— åç¼€) + "__backup_" + timestamp + åç¼€
      ä¾‹å¦‚ï¼šdata.xlsx -> data__backup_20250101_120000.xlsx
    - æ–‡ä»¶å¤¹ï¼šåŸæ–‡ä»¶å¤¹å + "__backup_" + timestamp
      ä¾‹å¦‚ï¼šfigures -> figures__backup_20250101_120000
    
    å‚æ•°:
        filepath: æºæ–‡ä»¶/æ–‡ä»¶å¤¹è·¯å¾„
        backup_dir: å¤‡ä»½ç›®å½•è·¯å¾„
    
    è¿”å›:
        å¤‡ä»½è·¯å¾„ï¼ˆæ–‡ä»¶/æ–‡ä»¶å¤¹ï¼‰ï¼Œå¤±è´¥åˆ™è¿”å› None
    """
    # æ£€æŸ¥æºè·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(filepath):
        print(f"âŒ [å¤‡ä»½å¤±è´¥] è·¯å¾„ä¸å­˜åœ¨: {filepath}")
        return None

    try:
        # ç¡®ä¿å¤‡ä»½æ ¹ç›®å½•å­˜åœ¨
        ensure_directory(backup_dir)
        
        # è§£ææºè·¯å¾„çš„åŸºç¡€åç§°ï¼ˆæ–‡ä»¶å/æ–‡ä»¶å¤¹åï¼‰
        base_name = os.path.basename(filepath)
        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # åŒºåˆ†å¤„ç†ï¼šæ–‡ä»¶ vs æ–‡ä»¶å¤¹
        if os.path.isfile(filepath):
            # å¤„ç†æ–‡ä»¶ï¼šæ‹†åˆ†åç¼€
            name, ext = os.path.splitext(base_name)
            backup_name = f"{name}__backup_{timestamp}{ext}"
            backup_path = os.path.join(backup_dir, backup_name)
            # å¤åˆ¶æ–‡ä»¶ï¼ˆä¿ç•™å…ƒæ•°æ®ï¼‰
            shutil.copy2(filepath, backup_path)
        elif os.path.isdir(filepath):
            # å¤„ç†æ–‡ä»¶å¤¹ï¼šæ— åç¼€ï¼Œç›´æ¥æ‹¼æ¥
            backup_name = f"{base_name}__backup_{timestamp}"
            backup_path = os.path.join(backup_dir, backup_name)
            # é€’å½’å¤åˆ¶æ–‡ä»¶å¤¹ï¼ˆå¤„ç†ç›®æ ‡å·²å­˜åœ¨çš„æƒ…å†µï¼‰
            if os.path.exists(backup_path):
                # è‹¥å¤‡ä»½æ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼Œè¿½åŠ éšæœºåç¼€é¿å…å†²çªï¼ˆå¯é€‰é€»è¾‘ï¼‰
                backup_path += f"_{os.urandom(4).hex()}"
            shutil.copytree(filepath, backup_path)
        else:
            print(f"âŒ [å¤‡ä»½å¤±è´¥] æ—¢ä¸æ˜¯æ–‡ä»¶ä¹Ÿä¸æ˜¯æ–‡ä»¶å¤¹: {filepath}")
            return None
        
        print(f"ğŸ“¦ [å¤‡ä»½æˆåŠŸ] {base_name} å·²å¤‡ä»½è‡³: {backup_path}")
        return backup_path

    except Exception as e:
        print(f"âŒ [å¤‡ä»½å¤±è´¥] {filepath} å¤‡ä»½å‡ºé”™: {str(e)}")
        return None